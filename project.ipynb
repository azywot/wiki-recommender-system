{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1guGiBfC0yDN"
      },
      "source": [
        "# 🌐 Wikipedia recommender system\n",
        "- Zuzanna Gawrysiak 148255\n",
        "- Agata Żywot 148258"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwT5cAGPNQEc"
      },
      "source": [
        "### Description (straight from ekursy - to be deleted later)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWarUy35PeJu"
      },
      "source": [
        "The general task is to create a system that will recommend similar articles based on the previously visited articles.\n",
        "\n",
        "**Input - Collection of articles (links or titles), Output - Collection of recommended articles (links or titles) with a \"score\"**\n",
        "\n",
        "\n",
        "You will receive a grade for each of the following steps. The highest possible score without\n",
        "finishing all parts is 4.0. For example, if you do perfectly the first two steps your grades will be 4.0,\n",
        "4.0, 2.0.\n",
        "\n",
        "\n",
        "**Crawling and scraping** - Download text from at least 1000 Wikipedia/fandom wiki articles.\n",
        "(Scrappy is not a must)\n",
        "\n",
        "\n",
        "**Stemming, lemmatization** - preprocess downloaded documents into the most suitable form for this\n",
        "task. Store it as a .csv/parquet file or into a database.\n",
        "\n",
        "\n",
        "**Similarities** - for a given collection of previously visited articles find the best matches in your\n",
        "database and recommend them to the user\n",
        "\n",
        "\n",
        "GUI not required, notebook or any other reasonable form will be accepted. I have to be able to\n",
        "provide a list of articles in an easy way and receive a meaningful recommendation.\n",
        "You have to send the source code and report.\n",
        "\n",
        "\n",
        "Report:\n",
        "- pdf or notebook\n",
        "- explain each step of your algorithm, especially how you score articles\n",
        "- present interesting statistics about your database (most frequent words, histograms, similarities\n",
        "between documents, ...)\n",
        "- show some examples of recommendations with explanations (I'd prefer graphical form - see\n",
        "prediction breakdowns for example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoOaTQDUUNrf"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5pvemV3Yuzk"
      },
      "outputs": [],
      "source": [
        "%pip install pyldavis\n",
        "%pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UTkL9HQUMqP",
        "outputId": "4936c6ad-80f4-436c-dae8-6ad686f49752"
      },
      "outputs": [],
      "source": [
        "# SCRAPPING\n",
        "import random\n",
        "import linecache\n",
        "import wikipedia\n",
        "import re\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cosine\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85dBFLXhOIKq"
      },
      "source": [
        "## Scraping wikipedia articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 100\n",
        "with open(r'./data/titles.txt', 'r') as fp:\n",
        "    num_lines = sum(1 for line in fp)\n",
        "    # print('Total lines:', num_lines) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get N article titles\n",
        "random.seed(2137)\n",
        "line_numbers = random.sample(range(1, num_lines), N)\n",
        "\n",
        "titles = []\n",
        "for i in line_numbers:\n",
        "    x = linecache.getline(r'./data/titles.txt', i).strip()\n",
        "    titles.append(x)\n",
        "print(titles[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_titles_content(titles):\n",
        "    \"\"\"\n",
        "    Create JSON files with all page information necessary\n",
        "    \"\"\"\n",
        "    for title in titles:\n",
        "        try: \n",
        "            page = wikipedia.page(title)\n",
        "        except:\n",
        "            print(f'No page of title {title}!')\n",
        "            continue \n",
        "        d = dict()\n",
        "        d['title'] = page.title\n",
        "        d['url'] = page.url\n",
        "        d['body'] = re.sub(r'==.*?==+', '', page.content)\n",
        "        d['links'] = page.links\n",
        "        try: \n",
        "            d['images'] = page.images\n",
        "        except:\n",
        "            d['images'] = ''\n",
        "         \n",
        "        json_object = json.dumps(d, indent=2)\n",
        "        file_title = re.sub(r'[\\s]','_', page.title)\n",
        "        file_title = re.sub(r'[,.-]','', file_title)\n",
        "        \n",
        "        with open(f'./data/pages_content/{file_title}.json', 'w') as outfile:\n",
        "            outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_titles_content(titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIGPZrPhPiR0"
      },
      "source": [
        "## Stemming, lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGXVpb5d1xdN"
      },
      "outputs": [],
      "source": [
        "def preprocess(article):\n",
        "    \"\"\"\n",
        "    Tokenize given article, remove stopwords, numbers, then perform stemming\n",
        "    \"\"\"\n",
        "    preprocessed = []\n",
        "    porter = PorterStemmer()\n",
        "    tokenized = word_tokenize(article)\n",
        "    sw = stopwords.words('english')\n",
        "\n",
        "    for word in tokenized:\n",
        "        if word.isalpha() and word not in sw:\n",
        "            preprocessed.append(porter.stem(word)) # stemming is faster than lematization, but has lower accuracy (can try both later)\n",
        "    return ' '.join(preprocessed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbM1EVQ6oQmx"
      },
      "source": [
        "### Count vector\n",
        "Store articles as numbers of occurences of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCJZeIp2ZOSh",
        "outputId": "1e553904-99f7-45e6-c456-f49f64f36424"
      },
      "outputs": [],
      "source": [
        "CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
        "CountData = CountVec.fit_transform(articles.body)\n",
        " \n",
        "CountData\n",
        "# if dataset is too large, try: https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "7RM55TZCaYks",
        "outputId": "47bf9439-b107-4485-bf61-7b41cf387e40"
      },
      "outputs": [],
      "source": [
        "dfCV = pd.DataFrame(CountData.toarray(), columns=CountVec.get_feature_names_out(), index=articles.URL)\n",
        "dfCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpe5d0i5p9WZ"
      },
      "source": [
        "## Database analysis\n",
        "> present interesting statistics about your database (most frequent words, histograms, similarities between documents, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyTFR9_FztzR"
      },
      "source": [
        "### Most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "KkWdljoDqRcE",
        "outputId": "fd7fc43e-8852-4bc0-f07b-14bd4935c100"
      },
      "outputs": [],
      "source": [
        "word_sums = dfCV.sum(axis=0)\n",
        "word_sums = word_sums.sort_values(ascending=False)\n",
        "print(f\"Top five most frequent words:\\n{word_sums[:5]}\")\n",
        "word_sums[:10].plot(kind='bar', figsize=(12,8), title=\"Most frequent words\", color='hotpink')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G3C8J1AIS6i"
      },
      "source": [
        "All words as a wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Cm-dVK1JIV_D",
        "outputId": "cc41c526-4e33-4203-cf61-7c9d2cbdc0c7"
      },
      "outputs": [],
      "source": [
        "def generate_wordcloud(data):\n",
        "    wc = WordCloud(width=1200, height=800, max_words=150, background_color='white', colormap='magma').generate_from_frequencies(data)\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "generate_wordcloud(word_sums)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaX3WOYMLBEN"
      },
      "source": [
        "### Similarities between documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OLDM0poObcP"
      },
      "source": [
        "Check the similarities using LDA (Latent Drichlet Allocation). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VEbedSB0LEf5",
        "outputId": "d848973a-9ff0-4b0f-8a5c-b462cefb32b6"
      },
      "outputs": [],
      "source": [
        "lda_tf = LatentDirichletAllocation(n_components=3, random_state=0) # number of articles\n",
        "lda_tf.fit(CountData)\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda_tf, CountData, CountVec, mds='tsne')\n",
        "panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAMIinMOKer9"
      },
      "source": [
        "### Conclusions from analysis\n",
        "* the most frequent word is ...\n",
        "* bla bla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAfXSFIGP4h2"
      },
      "source": [
        "## TFIDF approach\n",
        "Count vector was for showing some interesteing statistics, but for the recommendation we will use TFIDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "xWtEukueP8lX",
        "outputId": "9dfef1a4-a6c9-46e6-fb8e-e1c3e6377c43"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1), use_idf=True, smooth_idf=False, stop_words='english') \n",
        "tfidf_data = tfidf.fit_transform(articles.body) \n",
        "dfTFIDF = pd.DataFrame(tfidf_data.toarray(), index=articles.URL, columns=tfidf.get_feature_names_out())\n",
        "dfTFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpr-DToDh37C"
      },
      "source": [
        "Save the obtained data frame to csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgUtJPPqhcIg"
      },
      "outputs": [],
      "source": [
        "dfTFIDF.to_csv('articles.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HLIGoWtPouk"
      },
      "source": [
        "## Similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8cu2IOTPqv4",
        "outputId": "b8afa85f-18ad-4fde-8512-2fc339f2d953"
      },
      "outputs": [],
      "source": [
        "query = \"poznań\"\n",
        "query = preprocess(query)\n",
        "query = tfidf.transform([query]).toarray()[0] \n",
        "1-dfTFIDF.apply(lambda x: cosine(x, query), axis=1).sort_values()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HoOaTQDUUNrf"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 ('data-mining2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "e1791ee0b1058438a9e4dd50940b13379ea2066324df602589e5ef71bcea17c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
